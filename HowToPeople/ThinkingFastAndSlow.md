# _Thinking Fast and Slow_ - Notes üìù

This is a collection of quotes and notes taken from the book _"Thinking Fast and Slow"_, by Daniel
Kahneman.

## Two systems

Kahneman describes a dichotomy of two modes of thought - _system one_ and _system two_.

#### System 1Ô∏è

Fast, instinctive and emotional

- Solve 2 + 2 = ?
- Read text on billboard
- Understand simple sentences
- Drive car on an empty road
- Localize a source of sound
- Determine that an object is closer than another
- Responds more strongly to losses than to gains (_loss aversion_)
- exaggerates emotional consistency (_halo effect_)
- Links a sense of cognitive ease to illusions of truth, pleasant feelings, and reduced vigilance
- Can be _"programmed"_ by System 2 to mobilize attention when a particular pattern is detected
- Neglects ambiguity and suppresses doubt

#### System 2

Slower, more deliberate and logical

- Count the number of _'A's_ in a word
- Solve 17 \* 43 = ?
- Give someone your phone number
- Determine validity of a complex logical reasoning
- Park into a tight parking space
- Sustain higher than normal walking pace

Can we overcome cognitive illusions?

> The best that we can do is a compromise: learn to recognize situations in which mistakes are
> likely and try harder to avoid significant mistakes when the stakes are high. The premise of this
> book is that it is easier to recognize other people's mistakes on our own.

### Lazy controller üò¥

> We think as little as possible. The las of least effort is operating here.

The nervous system consumes more glucose than most other parts of the body, and effortful mental
activity appears to be especially expensive. When you are actively involved in a difficult cognitive
reasoning, or engaged in a task that requires self-control, your blood glucose level drops. The bold
implication of this idea is that **the effects of ego depletion could be undone by ingesting
glucose**.

> Volunteers in one of their studies watched a short silent film of a woman being interviewed and
> were asked to interpret her body language. While they were performing the task, a series of words
> crossed the screen in slow succession. The participants were specifically instructed to ignore the
> words, and if they found their attention drawn away they had to refocus their concentration on the
> woman‚Äôs behavior. This act of self-control was known to cause ego depletion. All the volunteers
> drank some lemonade before participating in a second task. The lemonade was sweetened with glucose
> for half of them and with Splenda for the others. Then all participants were given a task in which
> they needed to overcome an intuitive response to get the correct answer. Intuitive errors are
> normally much more frequent among ego-depleted people, and the drinkers of Splenda showed the
> expected depletion effect. On the other hand, the glucose drinkers were not depleted. Restoring
> the level of available sugar in the brain had prevented the deterioration of performance. It will
> take some time and much further research to establish whether the tasks that cause
> glucose-depletion also cause the momentary arousal that is reflected in increases of pupil size
> and heart rate.

### Cognitive strain and conative ease ‚òØÔ∏è

**Cognitive strain** occurs when our brain is doing some intense calculation, when we are reading
something complicated, or **when we are in a bad mood**.

Cognitive strain, whatever its source, mobilizes System 2, which his more likely to reject the
intuitive answer suggested be System 1.

When cognitive strain triggers System 2:

- We are more suspicious and **careful**
- We do not rely on intuition that much, thus we are less prone to make mistakes
- We are less creative

On the other hand, **cognitive ease** could be understood as the ease with which our mind processes
information.

> When you are in a state of cognitive ease, you are most likely in a good mood.

If you work in e.g. marketing you want people to be in the state of cognitive ease, as they are less
likely to be suspicious and vigilant.

Cognitive ease could be understood as the ease with which our minds process information.

üëâ A few tips for boosting the cognitive ease:

- Write your messages to be clear and easy to understand
- **Less is more!** Do not overwhelm your customer with too many options. See:
  [The paradox of choice](https://en.wikipedia.org/wiki/The_Paradox_of_Choice)
- If you are designing some process for people to go through (e.g. you're designing a system for
  placing purchase orders for an airline), try to keep it simple. Let it have as few steps as
  possible. Too many steps in a process may cause cognitive strain.
- Cognitive ease is about the feeling of familiarity. When you come across that is unfamiliar or
  unexpected we have to use more mental energy.

### Priming effect üî¢

Exposure to a stimulus influences a response to a subsequent stimulus, without conscious guidance or
intention.

[Check out Priming Effect on Wikipedia!](<https://en.wikipedia.org/wiki/Priming_(psychology)>)

You can use priming to your advantage and make people more open to your proposition/offer by
suggesting ideas beforehand.

### Halo effect üòá

**Exaggerated Emotional Coherence**

A positive impression of a person (or a brand/company) in one aspect will positive influence one's
opinion on other aspects. For instance, we tend to evaluate people highly on many traits because we
agree with theirs values or share their beliefs.

Customers are biased towards products of a company based on good experience with other procure made
by the same company. Halo effect can protect brands reputation in times of crisis.

Compare these two people:

- Adam is _intelligent, industrious, impulsive, critical, stubborn, envious_
- Ben is _envious, stubborn, critical, impulsive, industrious, intelligent_

More people ten to see Adam more favorably! **The halo effect increases the weight of first
impressions, often to a point where subsequent information is ignored.**

> The stubbornness of an intelligent person is seen as likely to be justified and may actually evoke
> respect, but intelligence in an envious and stubborn person makes him more dangerous. The halo
> effect is also an example of suppressed ambiguity: like the word bank, the adjective stubborn is
> ambiguous and will be interpreted in a way that makes it coherent with the context.

The [Horn effect](https://en.wikipedia.org/wiki/Horn_effect) is somewhat of a opposite to the Halo
effect. It is when we are negatively influenced towards a person or brand by a simple negative
experience.

### (WYSIATI) What You See Is All There Is üëÄ

People make judgement based on the information they have at the moment. We tend to not question
ourselves that _perhaps there are other facts that we're not aware of_. When we make decisions, our
brains use the facts and knowledge that we have. The quality and quantity of such "facts" is not
important. Our minds will try to create a coherent story out of what they have.

> **The measure of success for System 1 is the coherence of the story it manages to create. The
> amount and quality of the data on which the story is based are largely irrelevant.** When
> information is scarce, which is a common occurrence, System 1 operates as a machine for jumping to
> conclusions. Consider the following: ‚ÄúWill Mindik be a good leader? She is intelligent and
> strong...‚Äù An answer quickly came to your mind, and it was yes. You picked the best answer based
> on the very limited information available, but you jumped the gun. What if the next two adjectives
> were _corrupt and cruel_?

### Answering a different question - substituting questions ‚ùì

> If you can‚Äôt solve a problem, then there is an easier problem you can solve: find it.

People sometimes simplify difficult tasks. System 1 often _substitutes_ a difficult question for a
simpler, heuristic question.

| Target Question                                                         | Heuristic Question                                            |
| ----------------------------------------------------------------------- | ------------------------------------------------------------- |
| How much would you contribute to save an endangered species?            | How much emotion do I feel when I think of dying dolphins?    |
| How happy are you with your life these days?                            | How is my mood right now?                                     |
| How popular will the president be six months from now?                  | How popular is the president right now                        |
| This woman is running for the primary. How far will she go in politics? | Does this woman look like a political winner?                 |
| How should financial advisers who prey on the elderly be punished?      | How much anger do I feel when I think of financial predators? |

## Heuristics and biases

Extreme predictions and willingness to predict rare events from weak evidence are both
manifestations of System 1.

### Anchoring effect ‚öìÔ∏è

The anchoring effect describes our tendency to be influenced by irrelevant numbers. When presented
with higher/lower numbers, experimental subjects gave higher/lower responses.

The anchoring effect is an important concept to keep in mind during negotiations or considering
price.

We favor the first bit of information that we learn.

### Framing effect üñº

**Presenting the same information in different ways can evoke different reactions.** The statement
that _"the odds of surviving a surgery are 90%"_ is more reassuring than the equivalent statement
_"mortality withing one month of surgery is 10%"_. A similar case would be _"90% fat-free"_ is
perceived more positively than _"10% fat"_.

### Availability heuristic üíæ

It's a mental shortcut people make when making judgments about probability of events on the basis of
how easy it is to think of examples - _"it you can think of it, it has to be important"_.

It is easier to recall consequences of something, the greater we perceive these consequences to be.

> Because of the coincidence of the planes crashing last month, she now prefers to take the train.
> The risk hasn't really changed. That's an availability bias.

> Stokes cause almost twice as many deaths as all accidents combined, but 80% of respondents judged
> accidental death to be more likely.
>
> Death by accidents was judged to be more than 300 times more likely than death by diabetes, but
> the true ratio is 1:4.

> In today‚Äôs world, terrorists are the most significant practitioners of the art of inducing
> availability cascades. With a few horrible exceptions such as 9/11, the number of casualties from
> terror attacks is very small relative to other causes of death. Even in countries that have been
> targets of intensive terror campaigns, such as Israel, the weekly number of casualties almost
> never came close to the number of traffic deaths. The difference is in the availability of the two
> risks, the ease and the frequency with which they come to mind. Gruesome images, endlessly
> repeated in the media, cause everyone to be on edge. As I know from experience, it is difficult to
> reason oneself into a state of complete calm. **Terrorism speaks directly to System 1.**

Judging by representativeness has benefits. For instance - usually people who act friendly are in
fact friendly and young men are more likely to drive aggressively than elder women. In many cases
there is some truth in the stereotypes and predictions done on such basis might be correct. A
downside of representativeness is an excessive willingness to predict the occupance of unlikely
events. In the book Kahneman presents the following example: _you see a person reading the New York
Times on the New York subway. Which of the following is a better bet about the reading stranger: She
has a PhD or that she does not have a college degree?_ Representativeness would bet on the PhD, when
it is statistically more probable that such person would not have any degree. There would be many
nongraduate readers of the New York Times.

### Substitution and the Linda experiment üë©‚Äçüíº

[Formal fallacy](https://en.wikipedia.org/wiki/Formal_fallacy) (pol. _"B≈Çƒôdy logiczno-jƒôzykowe"_) -
is a pattern of reasoning rendered invalid by a flow in its logical structure. The argument itself
could have true premises, but still have a false conclusion.

[Conjunction fallacy](https://en.wikipedia.org/wiki/Conjunction_fallacy) - is a formal fallacy that
occurs when it is assumed that specific conditions are more probable than a single general one.

The **Linda problem** is an example of the conjunction fallacy.

> Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a
> student, she was deeply concerned with issues of discrimination and social justice, and also
> participated in anti-nuclear demonstrations.
>
> Which is more probable?
>
> - Linda is a bank teller.
> - Linda is a bank teller and is active in the feminist movement.

If we think of this problem in terms of Venn diagrams the first answer seems to be the correct one.
The set of "feminist bank tellers" is a subset of all bank tellers. Despite that, in experiment
about 85% or respondents choose the second option.

### Causes trump statistics

**Humans suck at Bayesian inference!** ü•µ

Consider the following task:

> A cab was involved in a hit-and-run accident at night. Two cab companies, the Green and the Blue,
> operate in the city. You are given the following data:
>
> - 85% of the cabs in the city are Green and 15% are Blue.
> - A witness identified the cab as Blue. The court tested the reliability of the witness under the
>   circumstances that existed on the night of the accident and concluded that the witness correctly
>   identified each one of the two colors 80% of the time and failed 20% of the time.
>
> What is the probability that the cab involved in the accident was Blue rather than Green?

This is a typical Bayesian inference problem. Let's try to solve it:

```
Let A represent the scenario where a blue cab caused the accident.
We know that P(A) is 0.15. Let B represent the scenario where the witness
recognized the color of the cab as blue. P(B|A) then indicate the probability
of the witness correctly identifying the color as blue. We know that it is 0.8.

What we want to find is the value of P(A|B) - that is the _probability of A provided that B_.

Using the Bayes theorem we know that it should be:

          P(B|A) * P(A)
P(A|B) = --------------
              P(B)

P(B) can be expressed as:

P(B) = P(B|A) * P(A) + P(B|not A) * P(not A) = 0.29

Plugin in values we get:

P(A|B) = 0.41
```

So the correct answer is 41%. However, most people would answer that it is 80%.

What's curious is that when the original task was changed to:

> The two companies operate the same number of cabs, but Green cabs are involved in 85% of
> accidents. The results people give are much more closer to the correct answer.

In the first task we are given a statistical fact. Our minds are hungry for for casual stories and
do not find nothing to _"chew on"_. _How does the number of green and blue cabs in the city cause
the cab driver to hit and run?_

The second version informs that Green cabs cause five times as many accidents. We get an immediate
conclusion: _"the Green drivers must be a collection of reckless madmen!"_ We now form a stereotype
of Green cab drivers being reckless. Now when we hear the witness' testimony about the blue cab we
have a conflict. The inferences from the two stories cancel each other out and we estimate that the
chances for the two colors are about equal.

#### Can psychology be taught? üôá‚Äç‚ôÇÔ∏è

> _"Subjects‚Äô unwillingness to deduce the particular from the general was matched only by their
> willingness to infer the general from the particular."_
>
> There is a deep gap between our thinking about statistics and our thinking about individual cases.
> Statistical results with a causal interpretation have a stronger effect on our thinking than
> noncausal information. But even compelling causal statistics will not change long-held beliefs or
> beliefs rooted in personal experience. On the other hand, surprising individual cases have a
> powerful impact and are a more effective tool for teaching psychology because the incongruity must
> be resolved and embedded in a causal story. That is why this book contains questions that are
> addressed personally to the reader. You are more likely to learn something by finding surprises in
> your own behavior than by hearing surprising facts about people in general.

### Remember about the regression to the mean! üìà

üí° Recall the story of Israeli Air Force instructors and their _criticism is more effective than
praise_. What they did not take into account was the regression to the mean!

> A characteristic of unbiased predictions is that they permit the prediction of rare or extreme
> events only when the information is very good. If you expect your predictions to be of modest
> validity, you will never guess an outcome that is either rare or far from the mean.

üëâ Extreme predictions and willingness to predict rare events are both manifestations of System 1!

### Narrative fallacy, social effects of hindsight, and the illusion of understanding üßô‚Äç‚ôÇÔ∏è

Narrative fallacies arise from our continuos attempt to make sense of the world. We fool ourselves
by constructing flimsy accounts of the past and believing them to be true. **A compelling story
fosters an illusion of inedibility.**

> The mind that makes up narratives about the past is a sense-making organ. When an unpredicted
> event occurs, we immediately adjust our view of the world to accommodate the surprise. Imagine
> yourself before a football game between two teams that have the same record of wins and losses.
> Now the game is over, and one team trashed the other. In your revised model of the world, the
> winning team is much stronger than the loser, and your view of the past as well as of the future
> has been altered be f—Äy that new perception. Learning from surprises is a reasonable thing to do,
> but it can have some dangerous consequences.

> **Hindsight bias** has pernicious effects on the evaluations of decision makers. It leads
> observers to assess the quality of a decision not by whether the process was sound but by whether
> its outcome was good or bad. Consider a low-risk surgical intervention in which an unpredictable
> accident occurred that caused the patient‚Äôs death. The jury will be prone to believe, after the
> fact, that the operation was actually risky and that the doctor who ordered it should have known
> better. This outcome bias makes it almost impossible to evaluate a decision properly‚Äîin terms of
> the beliefs that were reasonable when the decision was made.

### The illusion of validity ‚ú®

> Subjective confidence in a judgment is not a reasoned evaluation of the probability that this
> judgment is correct. Confidence is a feeling, which reflects the coherence of the information and
> the cognitive ease of processing it. It is wise to take admissions of uncertainty seriously, but
> declarations of high confidence mainly tell you that an individual has constructed a coherent
> story in his mind, not necessarily that the story is true.

üí° Recall **the illusion of stock-picking skill**.

> Mutual funds are run by highly experienced and hardworking professionals who buy and sell stocks
> to achieve the best possible results for their clients. Nevertheless, the evidence from more than
> fifty years of research is conclusive: for a large majority of fund managers, the selection of
> stocks is more like rolling dice than like playing poker. Typically at least two out of every
> three mutual funds underperform the overall market in any given year. More important, the
> year-to-year correlation between the outcomes of mutual funds is very small, barely higher than
> zero. The successful funds in any given year are mostly lucky; they have a good roll of the dice.
> There is general agreement among researchers that nearly all stock pickers, whether they know it
> or not‚Äîand few of them do‚Äîare playing a game of chance. The subjective experience of traders is
> that they are making sensible educated guesses in a situation of great uncertainty. In highly
> efficient markets, however, educated guesses are no more accurate than blind guesses.

üí°Remember: **Intuition cannot be trusted in the absence of stable regularities in the
environment.**

### People do not trust algorithms ü§ñ

People seem to have a negative prejudice against algorithms.

> (...) for most people, the cause of a mistake matters. The story of a child dying because an
> algorithm made a mistake is more poignant than the story of the same tragedy occurring as a result
> of human error, and the difference in emotional intensity is readily translated into a moral
> preference.

### Planning fallacy üìÜ

The planning fallacy is the tendency to overestimate benefits and underestimate costs.

#### Competition neglect

_"90% of drivers believe they are better than average."_

Kahneman tries to explain the causes of _entrepreneurial optimism_. He points to System 1 and
WYSIATI.

- We focus on our goal, anchor on our plan, and neglect relevant base rates, exposing ourselves to
  **the planning fallacy**.
- We focus on what we want to do and can do, neglecting the plans and skills of others.
- Both in explaining the past and in predicting the future, we focus on the causal role of skill and
  neglect the role of luck. We are therefore prone to an _illusion of control_.
- We focus on what we know and neglect what we do not know, which makes us overly confident in our
  beliefs.

**We compare ourselves to the average without thinking about average.**

## The prospect theory ‚öñÔ∏è

[The prospect theory](https://en.wikipedia.org/wiki/Prospect_theory) is build on top of two things.
First is the _loss aversion_ and the asymmetry in avoiding loss. Second, is the idea that people
attribute excessive wight to events with low probability and at the same time underestimate high
probability events.

[Prospect theory in one minute üé•](https://www.youtube.com/watch?v=sM91d5I36Po)

### Loss aversion üí∏

People tend to prefer avoiding losses to equivalent gains. In other words, we think that it is
better to avoid losing 10$, than winning 10$. Furthermore, a person who loses
10$ will lose more satisfaction than a person who won 10$.

The graph below taken from _Wikipedia_ presents the idea of loss aversion.

<div align="center">
<img width="400" src="./Loss_Aversion.png">
</div>

**We are more strongly driven towards avoiding losses than to achieve gains!**

> Loss aversion is a powerful conservative force that favors minimal changes from the status quo in
> the lives of both institutions and individuals. This conservatism helps keep us stable in our
> neighborhood, our marriage, and our job; it is the gravitational force that holds our life
> together near the reference point.

#### The endowment effect ‚òïÔ∏è

People are more likely to keep an object they poses rather than acquire it when they do not have it.
This asymmetry can be explained as loss aversion applied to ownership.

> For most things are differently valued by those who have them and by those who wish to get them:
> what belongs to us, and what we give away, always seems very precious to us.
>
> ~‚ÄâAristotle

In the book there is an experiment mentioned, in which participants were given a mug and then
offered to either sell it, or exchange it for an object of equivalent value. They found that the
price of the mug that people were willing to pay to acquire was twice lower than the compensation
for a mug expected by people who "owned" it.

## Humans are not Econs! üíÅ‚Äç‚ôÇÔ∏è

> The only test of rationality is not whether a person‚Äôs beliefs and preferences are reasonable, but
> whether they are internally consistent. A rational person can believe in ghosts so long as all her
> other beliefs are consistent with the existence of ghosts. A rational person can prefer being
> hated over being loved, so long as his preferences are consistent. Rationality is logical
> coherence‚Äîreasonable or not. Econs are rational by this definition, but there is overwhelming
> evidence that Humans cannot be. An Econ would not be susceptible to priming, WYSIATI, narrow
> framing, the inside view, or preference reversals, which Humans cannot consistently avoid.

## The remembering self and the experiencing self üñá

This is the idea Kahnemen describes as _two selves_. The _experiencing self_ asks _"How something
feels now"_, and the _remembering self_, _"How did it feel overhaul"_. Memories are the only thing
we get to keep from experiences, so the perspective that we adopt when thinking about our lives is
the _remembering self_.

Humans have a _duration neglect_. ‚è≥

> An inconsistency is built into the design of our minds. We have strong preferences about the
> duration of our experiences of pain and pleasure. We want pain to be brief and pleasure to last.
> **But our memory, a function of System 1, has evolved to represent the most intense moment of an
> episode of pain or pleasure (the peak) and the feelings when the episode was at its end. A memory
> that neglects duration will not serve our preference for long pleasure and short pains.**

> The remembering self is a construction of System 2. However, the distinctive features of the way
> it evaluates episodes and lives are characteristics of our memory. Duration neglect and the
> peak-end rule originate in System 1 and do not necessarily correspond to the values of System 2.
> We believe that duration is important, but our memory tells us it is not. The rules that govern
> the evaluation of the past are poor guides for decision making, because time does matter.
